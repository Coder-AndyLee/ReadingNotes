# 第一章 文字和语言 vs 数字和信息

- 消岐：上下文
- 翻译这件事之所以能达成，仅仅是因为不同的文字系统在记录信息的能力上是等价的。
- 信息的冗余是信息安全的保障。
  - 罗塞塔石碑的同一内容重复三次，只要有一份完好保留，则原有信息不会丢失。
  - 这对**信道编码**有指导意义。
- 语言的数据，我们称之为语料，尤其是双语或多语的对照语料对翻译至关重要。
- 计数系统和进制
  - 今天使用十进制的原因：早起祖先的计数系统是掰指头。
  - 玛雅文明使用二十进制，相比十进制复杂很多（如乘法表需要背19*19的棋盘），这可能也是玛雅文明发展缓慢的原因
- 对于不同位数数字的表示，中国人用个十百千万，罗马人用I II III IV等；这不自觉地引入了朴素的编码概念
  - 用不同的符号代表不同的数字概念
  - 制定了解码规则。
    - 中国：乘法，即 两百万 = 2 * 100 * 10000
    - 罗马：加减法，即 IV = 5 - 1 = 4，复杂很多。
    - 最有效的是古印度人 -> 阿拉伯数字。叫阿拉伯数字的原因是，这套表示方法是阿拉伯人传入欧洲的。
- 罗马文字中，常用字段、生僻字长，符合信息论中的最短编码原理。
- 古时，中文的口语移动且较长，但书写的文字却难懂且较短：书写材料稀少、书写成本大（刻字时间长）
  - 符合信息科学的原理：通信时，若信道较宽，信息不必压缩就可以直接传递；若信道很窄，信息在传递前要尽可能地压缩，然后在接收端进行解压缩。
- 犹太人抄写《圣经》时，为了避免抄写错误，使用类似**校验码**的方法。
  - 每个字母对应一个数字，校验码 = 该行所有数字之和；每一列同理。这样即可使用行列校验码定位出错位置
- 从字母到词的构词法是词的编码规则，语法则是语言的编码和解码规则。词是有限而封闭的集合，语言是无限而开放的集合。
  
  ​
# 第二章 自然语言处理 - 从规则到统计

- 早期对自然语言处理的误解：要让机器完成翻译或语音识别等只有人类才能做的事情，就必须先让计算机理解自然语言，而做到这一点就必须让计算机拥有类似人类的智能。
- 自然语言处理：**分析语句** 和 **获取语义**
- 自然语言理解：**句法分析** 和 **语义分析**
- 语法分析：任何线性的语句都可用二维的语法分析树（Parse Tree）来表示。
- 基于规则的语义分析
  - 瓶颈：词的多义性很难用规则来描述。
  - 主要是基于语义文法规则来进行语义分析
- 基于统计的语义分析
  - 20世纪70年代，基于统计的方法的核心模型是通信系统+隐含马尔科夫模型
  - 运用统计模型解决语义分析

# 第三章 统计语言模型

- 自然语言逐渐演变成一种**上下文相关的信息表达和传递方式**，所以处理自然语言，一个基本的问题是，为其建立数学模型，即统计语言模型（Statistical Language Model） -> 用于判断文字序列是否合乎文法、语义是否正确。
- 语言模型的数学表达
  - 文字序列是否合理的指标定义
    - 看它的可能性大小 -> 概率
    - 假设句子为S，每个单词为wi（i = 1, 2, ..., n)，则P(S) = P(w1, w2, ..., wn) 
    - P(w1, w2, ..., wn) = P(w1) * P(w2 | w1) * P(w3 | w1, w2) * ... * P(wn | w1, w2, ..., wn-1)
    - 马尔科夫假设：假设任意一个词wi只与它前一个词wi-1有关
      - P(w1, w2, ..., wn) = P(w1) * P(w2 | w1) * P(w3 | w2) * ... * P(wn | wn-1)
      - 二元模型（Bigram Model）。当一个词由前面n个词决定时，为n-gram model
      - 求解每项概率
      - ​

$$
P(w_i | w_{i-1}) = \frac{P(w_{i-1}, w_i)}{P(w_{i-1})}
$$
      - 联合概率P(wi-1, wi)和边缘概率P(wi-1)可用语料库估计
         - 即统计wi-1, wi这两个词在语料库中出现的次数，分别除以语料库大小，可得到它们的相对频度。根据大数定理，相对频度约等于概率。再拿二者概率相除即可得到结果
         - 

$$
P(w_i | w_{i-1}) = \frac{f(w_{i-1}, w_i)}{f(w_{i-1})}
$$

- 高阶语言模型
  - 假设句子中的每个词语前N-1个词有关 -> N-1阶马尔科夫假设 / N-Gram Model
  - 一般N = 1、2、3，再大开销会变得格外大，且带来的性能提升并不显著，这就是马尔科夫假设的局限性。原因：
    - 空间复杂度：O(V^N)
    - 时间复杂度：O(V^(N-1))
    - 需要采用一些**长程的依赖性（Long Distance Dependency）**来解决
- 模型训练
  - 参数：模型中所有的条件概率
  - 模型的训练：通过对语料的统计，得到这些条件概率的过程
- 零概率问题
  - 分子为0时，即两个词无贡献次数时，会导致某一项概率为零，而整体概率是每个条件概率的乘积，会导致整体概率为零；若两个单词都只出现一次，会导致P(wi | wi-1) = 1的情况。
  - 在实际应用中，统计语言模型的零概率问题是无法回避的。
  - Solutions
    - 增加数据量，但并不总是奏效
    - 古德 - 图灵估计
      - 对于没有看见的事件，我们不能认为它发生的概率就是零，因此我们从概率的总量（Probabilities Mass）中分配一个很小的比例给没有看见的事件。
      - 估计公式

# 第四章 谈谈分词

- 词是表达语义的最小单位
- 中文分词的发展
  - 查字典法：把句子从左到右扫描，遇到字典里有的就标识出来；遇到复合词找最长的词匹配，遇到未知词就分割成单个单词。
  - 查字典法优化 - 最少词数的分词理论：一句话应该分割成数量最少的词串。但对具有**二义性**的分割无能为力；且并非所有的最长匹配都是正确的。





